[IN CONSTRUCTION]
# Intro
NNs are powerful (in comp. to other ML algorithms) in nonlinear classification problem setting.

Stacking layer on one another is powerful itself, but stacking nonlinearities on nonlinearities lets us model very complicated relationships between the inputs and the predicted output.

* more [TODO]: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/

## Difference between DNN and traditional ML

![data-tradeoff-curve](./DNN-ML-tradeoff-curve.png)

## Reasons for underperformance
### Data is unbalanced
Solution: **Data Optimization**
- Subsampling of majority class
- Oversampling of minority class
Read more [here](http://amsantac.co/blog/en/2016/09/20/balanced-image-classification-r.html).

### There is little data
Solution: **Generate more**
- More [here](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)

### Hyperparameters are not properly optimized
- [Hyper-Parameter Optimization](./Hyperparameters.md)