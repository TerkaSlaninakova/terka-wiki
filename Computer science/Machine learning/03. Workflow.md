# Phases
There are many ways to approach any machine learning task. Some of them, encouraged by tutorials and a need for quick results leads to more bugs and headaches than less (= [**fast and furious” approach to training neural networks does not work**](https://karpathy.github.io/2019/04/25/recipe/), doesn't apply only to NNs)

## First steps:
- Dataset exploration - spend a lot of time here to really get feel for the training and testing dataset
- Set up end-to-end evaluation skeleton = toy model and get dumb baseline results (to beat them later)
- Find a good model (**Goal: Model performs well (at least) on training dataset**)
  - pick something verified (e.g. copy-paste ResNet-50 if classifying images)
  - build up the complexity slowly
- Regularize (**Goal: Model performs well on testing dataset**)
  - add more data, try data augmentation
  - Try smaller model size, smaller input dimensionality
  - decrease batch size
  - dropout, weight decay, early stopping
  - try a larger model
- Tune
  - hyper-parameter optimization preferrably with random search
  - use ensembles (pretty much guaranteed way to gain 2% of accuracy on anything)
  - leave it training for a longer time

## Data exploration
**KNOW YOUR DATASET**
- plot it, visualize it, histograms, most to least common
- look for distributions, patterns in data
- how much variation is there? how much noise?
- **pay attention to the outliers**
- Are there any duplicate examples? Missing values?
- Are the training and validation dataset similar?
- Are there missing feature values for a large number of observations?
- Are there any unexpected feature values?
- Any signs of data skew / under-overrepresentation?
- Any spottable feature correlations?

- Example answers
- higher education levels generally tend to correlate with a higher income bracket
  - solution: `data['income_bracket'].apply(lambda x: ">50K" in x).astype(int)`

## Feature selection
Feature selection is removing irrelevant features from the dataset. It's necessary to normalize the dataset first, a lot of the techniques are scale-dependent.
There are algorithms with built-in feature selection, such as random forests. As a stand-alone task, **variance thresholds** (unsupervised, impl. in scikit), [**correlation thresholds**](https://gist.github.com/Swarchal/881976176aaeb21e8e8df486903e99d6) and [**genetic algorithms**](https://pypi.org/project/deap/) (supervised) are used
- Exhaustive search: `2^n-1`
- Random search
- [mRMR](https://en.wikipedia.org/wiki/Minimum_redundancy_feature_selection)
  - want to minimize the redundancy of features while maximizing the relevancy

## Feature engineering
Feature engineering is transforming raw data into a feature vector. The features must be represented as a real-numbered vectors, since they need to be multiplied by weights.

- numeric values don't need special mapping, they're numeric already :)
- categorical values can be mapped by one-hot-encoding/multi-hot-encoding

### General tips:
- avoid rare features (that show up in the dataset < 5 times, e.g. id) - model can't learn to make predictions out of it
- prefer obvious meanings - helps in debugging
- don't use magic values (e.g. -1 for feature not being present, use a new feature is_present or something instead)

### Types of engineering

#### Scaling
Converting numeric value from its original range to `0 - 1` or `-1 - 1`. Another popular scaling tactic is to base it on standard deviation:
$$scaled = (value-mean)/stddev$$

**WHY?**
- helps GD converge quickly
- avoids the NaN trap
- helps to learn appropriate weights for each feature

#### Handling extreme outliers
E.g. 50 `roomsPerPerson` in Cali housing DS.
We want to minimize the influence of the extreme outliers. This can be done by **taking the log** of every value, but that still leaves a tail, so we could **clip** the max at an arbitrary value, e.g. 4.0

#### Bin features that don't make much sense as a numeric value
E.g. `latitude` in Cali housing DS, convert it into 11 bins and one-hot encode to a single feature
Example code:
```
def select_and_transform_features(source_df):
  LATITUDE_RANGES = zip(range(32, 44), range(33, 45))
  selected_examples = pd.DataFrame()
  selected_examples["median_income"] = source_df["median_income"]
  for r in LATITUDE_RANGES:
    selected_examples["latitude_%d_to_%d" % r] = source_df["latitude"].apply(
      lambda l: 1.0 if l >= r[0] and l < r[1] else 0.0)
  return selected_examples

selected_training_examples = select_and_transform_features(training_examples)
selected_validation_examples = select_and_transform_features(validation_examples)
```

#### In practice there are usually more problems with a dataset:
- omitted values
- duplicate examples
- bad labels
- bad feature values
- non-existent values

#### How to spot Problems in a dataset?
- Maximum and minimum
- Mean and median
- Standard deviation
- Histogram
- Correlation matrix

#### Are feature crosses needed?
Feature cross is a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together

```
# Divide longitude into 10 buckets.
bucketized_longitude = tf.feature_column.bucketized_column(
  longitude, boundaries=get_quantile_based_boundaries(
    training_examples["longitude"], 10))

# Divide latitude into 10 buckets.
bucketized_latitude = tf.feature_column.bucketized_column(
  latitude, boundaries=get_quantile_based_boundaries(
    training_examples["latitude"], 10))

long_x_lat = tf.feature_column.crossed_column(set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=1000)
```

### Entity embeddings
A way to deal with categorical variables. Involves encoding into numbers and assigining each value an embedding vector.

#### Why?
NNs are **not as prominent** when dealing with ML problems with **structured data**.
- Why? Because they assume they're approximating arbitrary continuous function. E.g. tree-based methods don't assume continuity.

**One-hot encoding** is oftentimes used to transform categorical features to continuous ones.
- Problems:
  - if input of high cardinality, a lot new features = a lot more comp. power required
  - treats the values indep. of one another and ignores relationships between them

#### How?
1. Initialize random embedding matrix of dim. `m x D`, `m` = n. of unique values in the column, `D` = dimension for representation (hyperparam, at most `m`, in which case it's one-hot enc.)
2. For each forward pass, do a lookup for the value, which'll give us vector `1 x D`
3. Append it to the input vector

#### Example of usefulness
[Kaggle competition of German grocery chain](https://www.kaggle.com/c/rossmann-store-sales) where they asked to predict the sales of items in their stores. And that included the mixture of categorical and continuous variables. In this competition there was **a feature representing how many months the competitor’s shop has been open**. In that case **zero months or one months is really different**.
So if you fed that in as a continuous variable, it would be **difficult for the neural net to try and find a functional form that has that big difference**.

## Model preparation/selection
- [TODO]: https://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/

## Training
### Loss
Loss is a penalty for a bad prediction. We have a parameter space, we can evaluate loss based on observations and predictions and we get some loss. Generally, when the loss is high, we might want to tune the parameters further to minimize it. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.

There are many types of loss functions:
#### Mead-squared error (also L2 loss)
$$
J(\theta) = \frac{1}{2n} \sum_{i=1}^n (h_\theta(x^i) - y^i)^2
$$

### Gradient descent
Gradient descent is a useful optimization algorithm, in which we make use of a direction of the loss function in the parameter space (its derivative). The negative gradient (symbolizing step downwards in a hill) tells us in which way to update the model parameters in order to minimize the loss.

GD for MSE:

$$
\frac{\partial}{\partial\theta_j} J(\theta) = \frac{1}{n} \sum_{i=1}^n (h_\theta(x^i) - y^i)^2 \tag{p. deriv. of MSE}
$$

$$
\theta_j = \theta_j - \alpha\dot\frac{\partial}{\partial\theta_j} J(\theta) \tag{GD update rule}
$$

Note:
It's useful to randomize the order of the training data before going into GD.

Types:
- vanilla GD = full batch:
```
for i in range(num_iters):
    [GD update rule for the whole dataset]
```
- SGD - GD is evaluated one training example at a time, which might take a lot of time in some cases:
```
for i in range(n):
    [GD update rule]
```
- minibatch - not one example at a time, but a batch at a time
```
for i in range(n):
    for j in range(minibatch_size): 
        [GD update rule]
```

**Learning rate** is an important hyperparameter here, represeting the length of the step of GD. Too small can take up many iterations to reach a local minimum, too large may overshoot the local minimum and fail to find it.

### Overfitting
is a failure to **generalize** well on new unseen data. Overfitting itself refers to ability to perform great on a training dataset (model overfit the data). The loss on training dataset will therefore be low, but the model would be bad in adapting to new data, which is the point of model training. Model usually has **high variance and low bias**

### Underfitting
is a failure to **capture the pattern of the data**. Models oversimplify and have **high bias, low variance**.

#### Causes
- model is more complex than necessary

#### Minigation stategies
- regularization techniques
- having proper [test-train split](#train-test-validation-split), cross-validation

#### Assumptions for good generalization
- examples drawn [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) at random
- distribution doesn't change within the dataset
- examples are drawn from the same distribution

### Train-test-validation split
**DO NOT TRAIN ON THE TEST DATA and BE WARY OF 100% ACCURACY**
The separation of testing and training dataset seems reasonable, but is itself not enough. Doing many rounds of this procedure might cause us to **implicitly fit to the peculiarities of our specific test set**. That's why **validation** dataset is used.

## Performance evaluation

## ML Engineering
**Feature management** - features effectively represent the 'code' in our machine learning system, therefore it's important to recognize which ones help/hurt our system (by the same analogy, there's no equivalent of unit-test for features)
  - is this signal (piece of data) reliable?
  - does the usefulness of this feature justify the cost of including it?
  - are there any correlations among features?

# Sources
- [ML carshcourse](https://developers.google.com/machine-learning/crash-course)
