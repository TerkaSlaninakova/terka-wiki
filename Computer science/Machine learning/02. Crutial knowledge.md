
## Core knowledge
* How to set up a model
* How to evaluate model's performance
    * metrics

## Phases
1. Data exploration
2. Feature selection/engineering
3. Model preparation/selection
4. Training
    - loss, its minimization, gradient descent
5. Performance evaluation

## Algorithms
* Regression Algorithms (relationships between variables)
    - Linear Regression (using Least Squares)
    - Logistic Regression (using Maximum likelihood)
* Clustering Algorithms (divide datapoints into groups w/ similar properties)
    - K-means
    - Hierarchical Clustering
* Dimensionality reduction algorithms (reduce n. of features to only what is necessary)
    - PCA
    - Low Variance Filter
    - High Correlation Filter
    - Random Forests
    - Backward Feature Elimination / Forward Feature construction
* Decision Trees
    - Classification and Regression Tree
    - C4.5 and C5.0
    - Random Forests
    - Chi-squared automatic interaction detector
* Deep NNs
    - Stacked Auto-encoders
    - CNNs
    - RNNs
    - Capsule Networks

* Combination of algorithms
    - bagging
    - stacking

## Data Exploration
- Variable Identification
- Uni-variate Analysis
- Bi-variate Analysis
- Multi-variate Analysis
## Feature Cleaning
- Missing Values
- Special Values
- Outliers
- Obvious inconsistencies
## Feature Imputation
- Hot-Deck
- Cold-Deck
- Mean-substitution
- Regression
## Feature Engineering
- Decomposition
- Dicretization
- Reframe Numerical Quantities
- Crossing
## Feature Selection
- Correlations
- Dimensionality Reduction
- Importance
## Feature Encoding
- Label Encoding
- One hot Encoding
## Feature Normalization
- Re-scaling
- Standardization
- Scaling to unit Length
## Dataset Construction
- Training Dataset
- Test Dataset
- Validation Dataset
- Cross validation

## Concepts
* Overfitting/Underfitting

## Answer the questions:
- Why did you choose this method to solve the problem?
- How did you handle missing data?
- Elaborate on issues you had during this project and how you solved it?

## Errors to avoid
- relying on the default loss function (MSE) for all the problems
- using only 1 model/algorithm
    - Let your data choose your model for you
- ignoring outliers
    - take a closer look at the data, determine if outliers should be ignored
- not standardizing the features
    - and then using e.g. L1/L2 regularization
- not understanding which features are important to the result

# Sources
- https://towardsdatascience.com/essential-algorithms-every-ml-engineer-needs-to-know-3167b1e940f