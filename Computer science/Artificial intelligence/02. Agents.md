# Agents
The job of AI is to design an agent program implementing agent function that takes percepts as an input and output actions while taking into account the architecture = physical sensors and actuators.

## Table-driven agent
```
function TABLE-DRIVEN-AGENT(percept) returns an action
persistent: 
    percepts: a sequence, initially empty
    table: set of actions indexed by percept sequences, initially fully specified
percepts.append(percept)
action ← LOOKUP(percepts, table)
return action
```
- won't work because lookup is too expesive, also the table structure would have to be huge

# Four basic kinds of agent programs - how is an action selected:

## 1. Simple reflex agent
- select action based on current percept, ignoring the percept history
- incorporates **condition-action rule**:
    - `if car-in-front-is-braking then initiate-braking`

```
function SIMPLE-REFLEX-AGENT(percept) returns an action
persistent: condition–action rules
state ← INTERPRET-INPUT(percept)
rule ← RULE-MATCH(state, rules)
action ← rule.ACTION
return action
```

These agents are simple, but of limited intelligence - the correct decision can be made only if the environment is fully observable.

## 2. Model-based reflex agent
- keep track of the unobservable aspects of the environment via **internal state** (e.g. driving agent needs to be able to predict the position of other cars).

```
function MODEL-BASED-REFLEX-AGENT(percept) returns an action
persistent: 
    state: agent's current conception of the env
    model: descr. of how the next state dep. on current state and action
    rules: condition-action rules
    action: most recent action

state ← UPDATE-STATE(state, action, percept, model)
rule ← RULE-MATCH(state, rules)
action ← rule.ACTION
return action
```

## 3. Goal-based agent
- as well as the current state description, agent needs some sort of **goal**

## 4. Utility-based agent
- goals are not enough to generate high-quality behavior in most environemnts
- introduce **utility function** - internalization of performance measure
- utility-based agents choose an action that maximize sexpected utility of the action outcomes

# Learning agents - how do agents improve thier performance?
- **learning element** - responsible for making improvements
    - feedback from **critic** - assesses how well is the agent doing w.r.t a fixed **performance standard**
- **performance element** - responsible for selecting external actions
- **problem generator** - suggests actions that will lead to new informative experiences